import re
import requests
import sys
import os
import hashlib
from typing import List, Dict, Optional, TYPE_CHECKING
import logging
from dataclasses import dataclass
from datetime import datetime
import time
import threading
from contextlib import contextmanager
if TYPE_CHECKING:
    from search_windows import AdvancedHybridSearch
 
# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# --- LLM model selection (Ollama tags) ---

# 1) –õ—ë–≥–∫–∏–π –∏ "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω—ã–π" (—Ä–µ–∫–æ–º–µ–Ω–¥—É—é –Ω–∞—á–∞—Ç—å —Å –Ω–µ–≥–æ)
# MODEL_DEFAULT = "qwen3:4b-instruct-2507-q4_K_M"
# 2) –°–∏–ª—å–Ω–µ–µ –Ω–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–∏ (–º–µ–¥–ª–µ–Ω–Ω–µ–µ)
# MODEL_DEFAULT = "qwen3:4b-thinking-2507-q4_K_M"
# MODEL_DEFAULT = "qwen3:4b-thinking-2507-q8_0"
# 3) –ë–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
# MODEL_DEFAULT = "qwen3:8b-q8_0"
# MODEL_DEFAULT = "qwen3:30b-a3b-instruct-2507-q4_K_M"
# 4) –°—Ç–∞—Ä—ã–π –≤–∞—Ä–∏–∞–Ω—Ç (–¥–ª—è –æ—Ç–∫–∞—Ç–∞)
MODEL_DEFAULT = "qwen2.5:7b-instruct-q5_K_M"

MODEL = os.getenv("LLM_MODEL", MODEL_DEFAULT)

OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/chat")
REQUEST_TIMEOUT = (10, 600)
KEEP_ALIVE = os.getenv("OLLAMA_KEEP_ALIVE", "10m")
MAX_FRAGMENTS = int(os.getenv("RAG_MAX_FRAGMENTS", "10"))         # —Å–∫–æ–ª—å–∫–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Ç—è–Ω–µ–º –∏–∑ RAG
MAX_GROUPS    = int(os.getenv("RAG_MAX_GROUPS", "5"))             # —Å–∫–æ–ª—å–∫–æ ¬´–≥—Ä—É–ø–ø –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤¬ª –≤ enhance_context
MAX_CITATIONS = int(os.getenv("RAG_CITATIONS_MAX", "8"))          # –º–∞–∫—Å–∏–º—É–º —Ü–∏—Ç–∞—Ç –≤ –æ—Ç–≤–µ—Ç–µ
MAX_CONTEXT_CHARS = int(os.getenv("RAG_MAX_CONTEXT_CHARS", "12000"))
MAX_CHARS_PER_FRAGMENT = int(os.getenv("RAG_CHARS_PER_FRAGMENT", "1500"))
NUM_PREDICT = int(os.getenv("LLM_NUM_PREDICT", "1024"))
NUM_CTX = int(os.getenv("LLM_NUM_CTX", "16384"))

# --- OKPD2 hint (no prompt injection) ---
OKPD_HINT_ENABLE = os.getenv("OKPD_HINT_ENABLE", "1") not in ("0", "false", "False")
OKPD_HINT_MIN_CONF = float(os.getenv("OKPD_HINT_MIN_CONF", "0.5")) # –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–ø–∏—Å–∫–∏ 
# --- Smart-Retry config ---
SMART_RETRY_ENABLE = os.getenv("SMART_RETRY_ENABLE", "1") not in ("0", "false", "False")
SMART_RETRY_MIN_HITS = int(os.getenv("SMART_RETRY_MIN_HITS", "3"))          # –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –º–µ–Ω—å—à–µ ‚Äî –ø—Ä–æ–±—É–µ–º –µ—â—ë —Ä–∞–∑
SMART_RETRY_TOPK_PER_QUERY = int(os.getenv("SMART_RETRY_TOPK_PER_QUERY", "5"))
SMART_RETRY_GROUNDING_THRESHOLD = float(os.getenv("SMART_RETRY_GROUNDING_THRESHOLD", "0.35"))

try:
    # –≤–∞—à –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä; –±–µ–∑–æ–ø–∞—Å–Ω–æ, –µ—Å–ª–∏ –º–æ–¥—É–ª—è –Ω–µ—Ç
    from scripts.utils.labels import classify_factory_item, okpd_canon  # noqa
except Exception:
    classify_factory_item = None  # type: ignore
    okpd_canon = None  # type: ignore

# —Å–∏–≥–Ω–∞–ª—ã ¬´–≤–æ–ø—Ä–æ—Å –ø—Ä–æ –û–ö–ü–î2¬ª
OKPD_INTENT_RE = re.compile(r"\b(–æ–∫–ø–¥2?|okpd)\b|\b–∫–æ–¥\s+–æ–∫–ø–¥2?\b", re.I)
SHAPE_RE  = re.compile(r"\b(–∫—Ä—É–≥|–∫–≤–∞–¥—Ä–∞—Ç|—à–µ—Å—Ç–∏–≥—Ä–∞–Ω–Ω–∏–∫|–ø–æ–ª–æ—Å–∞|–ª–∏—Å—Ç|–ª–µ–Ω—Ç–∞|–∫–∞—Ç–∞–Ω–∫–∞|–ø—Ä–æ–≤–æ–ª–æ–∫–∞|—Ç—Ä—É–±–∞|—É–≥–æ–ª–æ–∫|—à–≤–µ–ª–ª–µ—Ä|–¥–≤—É—Ç–∞–≤—Ä|–∞—Ä–º–∞—Ç—É—Ä–∞)\b", re.I)
MATERIAL_RE = re.compile(
    r"\b("
    r"—Å—Ç[0-9]+|—Å—Ç–∞–ª—å\s*\d+|aisi\s*\d+|"
    r"12[—Öx]18[–Ωnh]10[—Çt]|14[—Öx]17[–Ωnh]2|40[—Öx]13|30[—Öx]–≥—Å–∞|40[—Öx]|09–≥2—Å|65–≥|—Ä6–º5|[—Öx]12–º—Ñ|"
    r"–∞–º–≥\d?|1561|–∞–¥31|–ª[0-9]+|–ª—Å59|–ª63|–±—Ä[–∞-—è0-9]+|–º1|"
    r"(?:vt|–≤—Ç)\d?-?\d?"
    r")\b",
    re.I
)

def _okpd_detect_intent(q: str) -> bool:
    qn = (q or "").lower()
    if OKPD_INTENT_RE.search(qn):
        return True
    # —Ñ–æ—Ä–º–∞ + –º–∞—Ç–µ—Ä–∏–∞–ª ‚Äî —Ç–æ–∂–µ —Å—á–∏—Ç–∞–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–µ–º
    return bool(SHAPE_RE.search(qn) and MATERIAL_RE.search(qn))

def _okpd_extract_item(q: str) -> str:
    tail = re.split(r'(?i)\b(–¥–ª—è|–ø–æ|–Ω–∞)\b', q, maxsplit=1)
    tail = tail[-1] if len(tail) > 1 else q
    tail = re.sub(r'(?i)\b(–∫–∞–∫–æ–π|–Ω–∞–π–¥[–∏–π]|–ø–æ–¥—Å–∫–∞–∂–∏|–∫–æ–¥|–æ–∫–ø–¥2?|okpd)\b', ' ', tail)
    return re.sub(r'\s+', ' ', tail).strip()

class CtxLog(logging.LoggerAdapter):
    def process(self, msg, kwargs):
        prefix = []
        if self.extra.get("qa_id"): prefix.append(f"qa={self.extra['qa_id']}")
        if self.extra.get("cid"):   prefix.append(f"cid={self.extra['cid']}")
        return (("[" + " ".join(prefix) + "] " if prefix else "") + str(msg), kwargs)

@contextmanager
def timed(log, label: str):
    t0 = time.perf_counter()
    try:
        yield
    finally:
        log.debug(f"{label} finished in {(time.perf_counter()-t0)*1000:.1f} ms")

@dataclass
class PromptTemplate:
    """–®–∞–±–ª–æ–Ω—ã –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤"""
    
    SYSTEM_EXPERT = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ Falcon.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å —Ç–æ—á–Ω—ã–µ, –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–µ –∏ —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.

–ü–†–ò–ù–¶–ò–ü–´ –†–ê–ë–û–¢–´:
1. –¢–æ—á–Ω–æ—Å—Ç—å: –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
2. –°—Ç—Ä—É–∫—Ç—É—Ä–∞: –û—Ä–≥–∞–Ω–∏–∑—É–π –æ—Ç–≤–µ—Ç –ª–æ–≥–∏—á–Ω–æ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —Å–ø–∏—Å–∫–∞–º–∏
3. –ü—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç—å: –î–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏
4. –ß–µ—Å—Ç–Ω–æ—Å—Ç—å: –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —è–≤–Ω–æ –æ–± —ç—Ç–æ–º —Å–æ–æ–±—â–∏
5. –ö–æ–Ω—Ç–µ–∫—Å—Ç: –£—á–∏—Ç—ã–≤–∞–π –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
- –ò—Å–ø–æ–ª—å–∑—É–π Markdown –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- –ù–∞—á–∏–Ω–∞–π —Å –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ (1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Å–Ω–æ–≤–Ω—É—é —á–∞—Å—Ç—å —Å –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
- (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ó–∞–≤–µ—Ä—à–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –∏–ª–∏ —Å–ª–µ–¥—É—é—â–∏–º–∏ —à–∞–≥–∞–º–∏"""

    SYSTEM_ANALYTICAL = """–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –≥–ª—É–±–æ–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –Ω–µ –ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –Ω–æ –∏:
- –í—ã—è–≤–ª—è—Ç—å —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏
- –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –ø—Ä–µ–¥–ø–æ—Å—ã–ª–∫–∏
- –û–±—ä—è—Å–Ω—è—Ç—å –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
- –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞—Ç—å –æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö"""

    CHAIN_OF_THOUGHT = """–ü–æ–¥—É–º–∞–π –ø–æ—à–∞–≥–æ–≤–æ –∏ –ø—Ä–æ–≤–µ—Ä—å —Å–µ–±—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ, 
–ù–û –≤—ã–≤–æ–¥–∏ —Ç–æ–ª—å–∫–æ –≥–æ—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏, —Å–ø–∏—Å–∫–∞–º–∏, –ø—Ä–∏–º–µ—Ä–∞–º–∏."""

    FEW_SHOT_EXAMPLES = """"""

class EnhancedRAGChat:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π grounding"""
    
    def __init__(self, searcher: Optional['AdvancedHybridSearch'] = None):
        self.templates = PromptTemplate()
        self.conversation_history = []
        self.context_cache = {}
        self.searcher = searcher  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ searcher
        self._lock = threading.RLock()

    def classify_question(self, question: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        question_lower = question.lower()
        
        patterns = {
            'howto': ['–∫–∞–∫', '–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '—Å–ø–æ—Å–æ–±', '–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å', '—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å', '—Å–æ–∑–¥–∞—Ç—å'],
            'troubleshoot': ['–æ—à–∏–±–∫–∞', '–ø—Ä–æ–±–ª–µ–º–∞', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–∏—Å–ø—Ä–∞–≤–∏—Ç—å', '—Ä–µ—à–µ–Ω–∏–µ', '–ø–æ—á–µ–º—É'],
            'explain': ['—á—Ç–æ —Ç–∞–∫–æ–µ', '–æ–±—ä—è—Å–Ω–∏', '–∑–∞—á–µ–º', '–ø–æ—á–µ–º—É', '–ø—Ä–∏–Ω—Ü–∏–ø', '—Ä–∞–∑–ª–∏—á–∏–µ'],
            'reference': ['—Å–ø–∏—Å–æ–∫', '–ø–∞—Ä–∞–º–µ—Ç—Ä—ã', '—Ñ—É–Ω–∫—Ü–∏–∏', '–º–µ—Ç–æ–¥—ã', '—Å–≤–æ–π—Å—Ç–≤–∞', '–∫–æ–º–∞–Ω–¥—ã'],
            'compare': ['—Å—Ä–∞–≤–Ω–∏', '–æ—Ç–ª–∏—á–∏–µ', '—Ä–∞–∑–Ω–∏—Ü–∞', '–ª—É—á—à–µ', '–≤—ã–±—Ä–∞—Ç—å', '–∏–ª–∏']
        }
        
        for q_type, keywords in patterns.items():
            if any(kw in question_lower for kw in keywords):
                return q_type
        
        return 'general'
    
    def check_answer_grounding(
        self,
        answer: str,
        chunks: List[Dict],
        *,
        min_sent_len: int = 20,
        pool_max: int = 8,
        fuzz_threshold: int = 70
    ) -> float:
        """–û—Ü–µ–Ω–∫–∞ –¥–æ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –æ—Ç–≤–µ—Ç–∞, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏."""
        if not chunks or not answer:
            return 0.0

        # 1) –û—á–∏—Å—Ç–∫–∞ —Ä–∞–∑–º–µ—Ç–∫–∏/—à—É–º–∞
        answer_clean = re.sub(r"```.*?```", "", answer, flags=re.S)
        answer_clean = re.sub(r"`[^`]+`", "", answer_clean)                      # inline code
        answer_clean = re.sub(r"\[([^\]]+)\]\([^)]+\)", r"\1", answer_clean)     # [text](url)
        answer_clean = re.sub(r"https?://\S+", "", answer_clean)                 # bare urls
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–∏–ø–∏—Å–∫—É —Å –ø–æ–¥—Å–∫–∞–∑–∫–æ–π OKPD
        answer_clean = re.sub(r"(?m)^\s*>\s*‚ÑπÔ∏è\s*–ü–æ–¥—Å–∫–∞–∑–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞[^\n]*\n?", "", answer_clean)

        # 2) –°–ø–ª–∏—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è + –ª—ë–≥–∫–∞—è –∑–∞—á–∏—Å—Ç–∫–∞ –±—É–ª–ª–µ—Ç–æ–≤
        sentences = [
            s.strip(" \t\n\r-‚Äì‚Äî‚Ä¢*") for s in re.split(r'(?<=[.!?‚Ä¶])\s+', answer_clean) if s.strip()
        ]

        total = 0
        covered = 0

        def _norm(s: str) -> str:
            s = s.lower().replace('—ë', '–µ')
            s = re.sub(r"\s+", " ", s)
            return s.strip()

        # 3) –ü—É–ª –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π)
        pool_texts = [_norm(c.get('text', ''))[:1200] for c in chunks[:pool_max] if c.get('text')]
        if not pool_texts:
            return 0.0

        # 4) –ü—Ä–µ–¥–∫–æ–º–ø–∏–ª—è—Ü–∏—è –∏ —Ç–æ–∫–µ–Ω—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        tok_re = re.compile(r'[–∞-—èa-z0-9]{3,}', re.I)
        pool_tokens = set()
        for t in pool_texts:
            pool_tokens.update(tok_re.findall(t))

        # 5) –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å rapidfuzz
        try:
            from rapidfuzz import process, fuzz  # type: ignore
            have_rf = True
        except Exception:
            have_rf = False

        for sent in sentences:
            if len(sent) < min_sent_len:
                continue
            total += 1

            s = _norm(sent)
            kws = set(tok_re.findall(s))

            # 5.1 –ë—ã—Å—Ç—Ä–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
            if kws:
                overlap = len(kws & pool_tokens) / max(1, len(kws))
                if overlap >= 0.5:
                    covered += 1
                    continue

            # 5.2 –¢–æ—á–Ω–∞—è –ø–æ–¥—Å—Ç—Ä–æ–∫–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            if len(s) <= 220 and any(s in src for src in pool_texts):
                covered += 1
                continue

            # 5.3 Fuzzy –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –ø–æ—Ä–æ–≥–æ–º
            if have_rf:
                thr = fuzz_threshold
                if len(s) < 80:
                    thr += 10
                elif len(s) > 200:
                    thr -= 5
                thr = max(60, min(90, thr))

                try:
                    match = process.extractOne(s, pool_texts, scorer=fuzz.partial_token_set_ratio)
                    if match and match[1] >= thr:
                        covered += 1
                except Exception:
                    pass

        # 6) –ò—Ç–æ–≥
        if total == 0:
            return 0.0
        return covered / total

    def enhance_context(self, chunks: List[Dict], question: str) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π"""
        if not chunks:
            return ""
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –∏ —Ä–∞–∑–¥–µ–ª–∞–º
        grouped = {}
        for chunk in chunks:
            key = (chunk.get('book', 'Unknown'), chunk.get('section', 'Unknown'))
            if key not in grouped:
                grouped[key] = []
            grouped[key].append(chunk)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≥—Ä—É–ø–ø—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        sorted_groups = sorted(
            grouped.items(),
            key=lambda x: max(c.get('score', 0) for c in x[1]),
            reverse=True
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts = []

        for (book, section), group_chunks in sorted_groups[:MAX_GROUPS]:
            context_parts.append(f"\n### {book} - {section}\n")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
            combined_text = self._merge_overlapping_chunks(group_chunks)
            
            # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
            for text_part in combined_text:
                context_parts.append(
                    f"{text_part[:MAX_CHARS_PER_FRAGMENT]}\n"
                )

        return "\n".join(context_parts)
    
    def _merge_overlapping_chunks(self, chunks: List[Dict]) -> List[str]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (—É—Å—Ç–æ–π—á–∏–≤–æ –∫ page=None)."""
        if not chunks:
            return []
        
        def _is_parent(c: Dict) -> bool:
            return (c.get('is_parent') is True) or ((c.get('meta') or {}).get('is_parent') is True)

        # —É–±—Ä–∞—Ç—å —Ä–æ–¥–∏—Ç–µ–ª–µ–π
        pure = [c for c in chunks if not _is_parent(c)]
        if not pure:
            return [(c.get('text') or "") for c in chunks if c.get('text')]

        def _page_key(x: Dict) -> tuple:
            p = x.get('page', None)
            # None —É–≤–æ–¥–∏–º –≤ –∫–æ–Ω–µ—Ü
            return (p is None, p if isinstance(p, int) else 10**9, x.get('id', 0))

        sorted_chunks = sorted(pure, key=_page_key)
        if not sorted_chunks:  # –¥–æ–ø. —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞
            return [ (c.get('text') or "") for c in chunks if c.get('text') ]

        current_text = sorted_chunks[0].get('text', '') or ''
        current_page = sorted_chunks[0].get('page')
        current_page = current_page if isinstance(current_page, int) else None

        merged = []

        for chunk in sorted_chunks[1:]:
            p = chunk.get('page')
            p = p if isinstance(p, int) else None

            # —Å–∫–ª–µ–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–±–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑–≤–µ—Å—Ç–Ω—ã –∏ —Ä–∞–∑–Ω–∏—Ü–∞ <= 1
            if (current_page is not None) and (p is not None) and (p - current_page <= 1):
                overlap = self._find_overlap(current_text, chunk.get('text', '') or '')
                if overlap and len(overlap) > 50:
                    current_text = current_text + (chunk.get('text', '') or '')[len(overlap):]
                else:
                    current_text = current_text + "\n\n" + (chunk.get('text', '') or '')
                current_page = p
            else:
                merged.append(current_text)
                current_text = (chunk.get('text', '') or '')
                current_page = p

        merged.append(current_text)
        return merged

    def _find_overlap(self, text1: str, text2: str) -> str:
        """–ü–æ–∏—Å–∫ –æ–±—â–µ–π —á–∞—Å—Ç–∏ –º–µ–∂–¥—É –∫–æ–Ω—Ü–æ–º text1 –∏ –Ω–∞—á–∞–ª–æ–º text2"""
        max_overlap = min(200, len(text1), len(text2))
        
        for i in range(max_overlap, 20, -1):
            if text1[-i:] == text2[:i]:
                return text2[:i]
        return ""
    
    def build_enhanced_prompt(self, question: str, context: str, q_type: str) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞"""
        
        base_prompt = f"""–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–ò:
{context}

–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {question}

{self.templates.CHAIN_OF_THOUGHT}

–¢–í–û–ô –û–¢–í–ï–¢:"""
        
        type_instructions = {
            'howto': """""",
            
            'troubleshoot': """
–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê –î–õ–Ø –†–ï–®–ï–ù–ò–Ø –ü–†–û–ë–õ–ï–ú–´:
1. –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
2. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã (–æ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö)
3. –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ (–∫–∞–∫ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞–∂–¥—É—é –ø—Ä–∏—á–∏–Ω—É)
4. –†–µ—à–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –ø—Ä–∏—á–∏–Ω—ã
5. –ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞ –Ω–∞ –±—É–¥—É—â–µ–µ""",
            
            'explain': """
–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê –î–õ–Ø –û–ë–™–Ø–°–ù–ï–ù–ò–Ø:
1. –ü—Ä–æ—Å—Ç–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (ELI5)
2. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
3. –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã/–∞—Å–ø–µ–∫—Ç—ã
4. –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
5. –°–≤—è–∑—å —Å –¥—Ä—É–≥–∏–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏""",
            
            'reference': """
–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê –î–õ–Ø –°–ü–†–ê–í–ö–ò:
1. –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
2. –¢–∞–±–ª–∏—Ü–∞/—Å–ø–∏—Å–æ–∫ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
3. –û–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
4. –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
5. –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è""",
            
            'compare': """
–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê –î–õ–Ø –°–†–ê–í–ù–ï–ù–ò–Ø:
1. –ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥
2. –¢–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
3. –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π
4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É
5. –ü—Ä–∏–º–µ—Ä—ã —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        }
        
        if q_type in type_instructions:
            base_prompt = base_prompt.replace(
                "–¢–í–û–ô –û–¢–í–ï–¢:",
                type_instructions[q_type] + "\n\n–¢–í–û–ô –û–¢–í–ï–¢:"
            )
        
        return base_prompt
    
    def post_process_answer(self, answer: str, chunks: List[Dict]) -> str:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞: –æ—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        if not answer:
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
        
        # –£–¥–∞–ª—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
        answer = re.sub(r'–®–ê–ì \d+:.*?\n', '', answer)
        answer = re.sub(r'(–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é|–î—É–º–∞—é|–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é|–ü—Ä–æ–≤–µ—Ä—è—é).*?\n', '', answer)
        
        # –£–ª—É—á—à–∞–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞
        answer = re.sub(r'```(\w+)?\n', r'```\1\n', answer)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        if len(answer) < 100:
            answer = self._expand_short_answer(answer, chunks)
        
        return answer
    
    def _expand_short_answer(self, answer: str, chunks: List[Dict]) -> str:
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        if not chunks:
            return answer
        
        expanded = answer + "\n\n### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n"
        
        for chunk in chunks[:2]:
            text = chunk.get('text', '')[:200]
            if text:
                expanded += f"\n{text}...\n"
                expanded += f"*–°–º. {chunk.get('book', '')}, {chunk.get('section', '')}, —Å—Ç—Ä. {chunk.get('page', 0)}*\n"
        
        return expanded
    
    def generate_answer(
            self, question: str, top_k: int = None, use_cot: bool = True,
            chunks: Optional[List[Dict]] = None, *, qa_id: Optional[str] = None,
            conversation_id: Optional[str] = None
        ) -> Dict:
        """–ü—Ä–∏–Ω–∏–º–∞–µ–º chunks –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä"""
        start_time = datetime.now()

        if top_k is None:
            top_k = MAX_FRAGMENTS

        log = CtxLog(logger, {"qa_id": qa_id, "cid": conversation_id})
        log.debug(f"generate_answer start: top_k={top_k}, use_cot={use_cot}, chunks_in={len(chunks) if chunks else 0}")
        
        # OKPD2: –º—è–≥–∫–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏ (–Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ–º–ø—Ç)
        okpd_hint = None
        okpd_item = None
        if OKPD_HINT_ENABLE and _okpd_detect_intent(question):
            okpd_item = _okpd_extract_item(question)
            if okpd_item and len(okpd_item) > 255:
                okpd_item = okpd_item[:255]
            if classify_factory_item:
                try:
                    code, conf, desc, info = classify_factory_item(okpd_item)  # type: ignore
                    if code and code != "UNSURE":
                        if okpd_canon:
                            code = okpd_canon(code) or code  # type: ignore
                        okpd_hint = {"item": okpd_item, "code": code, "conf": float(conf or 0.0), "desc": desc or "", "info": info or {}}
                        log.info(f"okpd_hint=True item='{okpd_item}' code={code} conf={conf:.2f}")
                except Exception as e:
                    log.warning(f"okpd_hint_classifier_error: {e}")
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å
        q_type = self.classify_question(question)
        logger.info(f"–¢–∏–ø –≤–æ–ø—Ä–æ—Å–∞: {q_type}")
        log.info(f"–¢–∏–ø –≤–æ–ø—Ä–æ—Å–∞: {q_type}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ chunks –ò–õ–ò –∏—â–µ–º —Å–∞–º–∏
        if chunks is None:
            if not self.searcher:
                logger.error("–ù–µ—Ç –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –¥–≤–∏–∂–∫–∞ –∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã chunks")
                log.error("–ù–µ—Ç searcher –∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã chunks")
                return {
                    "answer": "–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫.",
                    "citations": [],
                    "error": True,
                    "processing_time": (datetime.now() - start_time).total_seconds()
                }
            
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏–∫–ª–∏—á–µ—Å–∫–æ–π –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
            logger.info("–í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π searcher")
            # –í–ê–ñ–ù–û: –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ 'item' (–ø–æ–¥—Å–∫–∞–∑–∫—É),
            #        –Ω–æ –ù–ï –º–µ–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç LLM.
            search_q = f"{question} {okpd_item}" if okpd_item else question
            # 1-–π –ø—Ä–æ–≥–æ–Ω: –æ–±—ã—á–Ω—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
            chunks = self.searcher.search(search_q, top_k=top_k, context_window=1)
            # Smart-retry: –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –º–∞–ª–æ ‚Äî –¥–µ–ª–∞–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º
            if SMART_RETRY_ENABLE and (not chunks or len(chunks) < SMART_RETRY_MIN_HITS):
                q_norm = normalize_for_retrieval(search_q)
                exp = build_multiquery_expansions(q_norm)
                retry = retrieve_and_rerank(self.searcher, exp, topk_per_query=SMART_RETRY_TOPK_PER_QUERY)
                if retry:
                    chunks = retry[:top_k]
        else:
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ chunks: {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            log.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ chunks: {len(chunks)}")

        if not chunks:
            log.warning("–ü—É—Å—Ç—ã–µ chunks ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º not-found")
            return {
                "answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.",
                "citations": [],
                "question_type": q_type,
                "processing_time": (datetime.now() - start_time).total_seconds()
            }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = self.enhance_context(chunks, question)
        if len(context) > MAX_CONTEXT_CHARS:
            log.info(f"Context {len(context)} > budget {MAX_CONTEXT_CHARS}, truncate")
            logger.info("–ö–æ–Ω—Ç–µ–∫—Å—Ç %d chars > budget %d, –ø–æ–¥—Ä–µ–∑–∞–µ–º", len(context), MAX_CONTEXT_CHARS)
            context = context[:MAX_CONTEXT_CHARS]
        log.debug(f"context_size={len(context)}")

        # –í—ã–±–∏—Ä–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        system_prompt = (
            self.templates.SYSTEM_ANALYTICAL 
            if q_type in ['explain', 'compare'] 
            else self.templates.SYSTEM_EXPERT
        )
        
        if use_cot:
            system_prompt += "\n\n" + self.templates.FEW_SHOT_EXAMPLES
        
        # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–º–ø—Ç (–ë–ï–ó –ø–æ–¥—Å–∫–∞–∑–∫–∏ OKPD!)
        user_prompt = self.build_enhanced_prompt(question, context, q_type)
        
        temp_map = {
            'reference': 0.3,
            'compare': 0.4,
            'howto': 0.4,
            'troubleshoot': 0.4,
            'explain': 0.5,
            'general': 0.45
        }
        temperature = temp_map.get(q_type, 0.45)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º keep_alive
        payload = {
            "model": MODEL,
            "stream": False,
            "keep_alive": KEEP_ALIVE,
            "options": {
                "num_ctx": NUM_CTX,
                "temperature": temperature,
                "top_p": 0.9,
                "repeat_penalty": float(os.getenv("LLM_REPEAT_PENALTY", "1.15")),
                "num_predict": NUM_PREDICT,
                "seed": 42
            },
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        }
        log.info(f"LLM request: model={MODEL}, num_ctx={NUM_CTX}, num_predict={NUM_PREDICT}, "
             f"temperature={temperature}, keep_alive={KEEP_ALIVE}")
        log.debug(f"messages={len(payload['messages'])}, context_chars={len(context)}, question_chars={len(question)}")

        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        with self._lock:
            hist = list(self.conversation_history[-4:])
        if hist:
            for msg in hist:
                payload["messages"].insert(1, msg)
        
        try:
            # –ó–∞–ø—Ä–æ—Å –∫ Ollama
            response = requests.post(
                OLLAMA_URL,
                json=payload,
                timeout=REQUEST_TIMEOUT
            )
            log.info(f"LLM status={response.status_code}, elapsed‚âà{response.elapsed.total_seconds():.3f}s")
            if response.status_code >= 400:
                log.error(f"LLM error body (first 500): {response.text[:500]}")
            response.raise_for_status()
            
            data = response.json()
            raw_answer = (
                data.get("message", {}).get("content")
                or data.get("response")
                or ""
            )
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
            with timed(log, "post_process_answer"):
                final_answer = self.post_process_answer(raw_answer, chunks)
            log.debug(f"final_answer_len={len(final_answer)}")
            
            # –î–æ–±–∞–≤–∏–º –Ω–µ–Ω–∞–≤—è–∑—á–∏–≤—É—é –ø—Ä–∏–ø–∏—Å–∫—É —Å –ø–æ–¥—Å–∫–∞–∑–∫–æ–π, –µ—Å–ª–∏ LLM —Å–∞–º –Ω–µ –Ω–∞–∑–≤–∞–ª —ç—Ç–æ—Ç –∫–æ–¥
            if okpd_hint:
                code_str = str(okpd_hint.get("code", "") or "")
                if code_str and code_str not in final_answer:
                    desc = (okpd_hint.get('desc','') or '').replace('\n', ' ')[:180]
                    if okpd_hint and okpd_hint["conf"] >= OKPD_HINT_MIN_CONF:
                        tip = (
                            f"\n\n> ‚ÑπÔ∏è –ü–æ–¥—Å–∫–∞–∑–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –û–ö–ü–î2: "
                            f"**{code_str}** ({okpd_hint.get('conf', 0.0):.0%}) ‚Äî {desc}."
                        )
                        final_answer = final_answer + tip
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ grounding
            with timed(log, "check_answer_grounding"):
                grounding_score = self.check_answer_grounding(final_answer, chunks)
            log.info(f"Grounding score: {grounding_score:.2%}")
            logger.info(f"Grounding score: {grounding_score:.2%}")

            # –í—Ç–æ—Ä–æ–π —à–∞–Ω—Å: –µ—Å–ª–∏ grounding —Å–ª–∞–±—ã–π ‚Äî —Ä–∞—Å—à–∏—Ä—è–µ–º –ø–æ–∏—Å–∫ –∏ –ø–µ—Ä–µ-—Å–ø—Ä–∞—à–∏–≤–∞–µ–º LLM
            if SMART_RETRY_ENABLE and grounding_score < SMART_RETRY_GROUNDING_THRESHOLD and self.searcher:
                context_used = context
                q_norm = normalize_for_retrieval(question + (" " + (okpd_item or "")))
                exp = build_multiquery_expansions(q_norm)
                retry = retrieve_and_rerank(self.searcher, exp, topk_per_query=SMART_RETRY_TOPK_PER_QUERY)
                if retry:
                    context2 = self.enhance_context(retry[:top_k], question)
                    if len(context2) > MAX_CONTEXT_CHARS:
                        context2 = context2[:MAX_CONTEXT_CHARS]
                    user_prompt2 = self.build_enhanced_prompt(question, context2, q_type)
                    payload["messages"][-1]["content"] = user_prompt2
                    try:
                        response2 = requests.post(OLLAMA_URL, json=payload, timeout=REQUEST_TIMEOUT)
                        response2.raise_for_status()
                        data2 = response2.json()
                        raw_answer2 = data2.get("message", {}).get("content") or data2.get("response") or ""
                        final_answer2 = self.post_process_answer(raw_answer2, retry)
                        grounding_score2 = self.check_answer_grounding(final_answer2, retry)
                        log.info(f"Grounding (retry): {grounding_score2:.2%}")
                        if grounding_score2 > grounding_score:
                            final_answer, grounding_score, chunks = final_answer2, grounding_score2, retry
                            context_used = context2
                    except Exception as e:
                        log.warning(f"LLM retry error: {e}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é (–ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ)
                with self._lock:
                    self.conversation_history.extend([
                        {"role": "user", "content": question},
                        {"role": "assistant", "content": final_answer}
                    ])
                    if len(self.conversation_history) > 10:
                        self.conversation_history = self.conversation_history[-10:]

                try:
                    context_len = len(context_used)
                except NameError:
                    context_len = len(context)

                # –§–æ—Ä–º–∏—Ä—É–µ–º —Ü–∏—Ç–∞—Ç—ã
                citations = []
                seen = set()
                def _is_parent(c: Dict) -> bool:
                    return (c.get('is_parent') is True) or ((c.get('meta') or {}).get('is_parent') is True)

                # –ë–µ—Ä—ë–º —á—É—Ç—å –±–æ–ª—å—à–∏–π –ø—É–ª –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –¥–µ—Ç–∏, –ø–æ—Ç–æ–º —Ä–æ–¥–∏—Ç–µ–ª–∏
                pool = sorted(chunks[:MAX_CITATIONS * 2], key=lambda x: _is_parent(x))

                for c in pool:
                    key = (c.get("book", ""), c.get("section", ""), c.get("page", 0))
                    # –µ—Å–ª–∏ –∫–ª—é—á —É–∂–µ –∑–∞–Ω—è—Ç —Ä–µ–±—ë–Ω–∫–æ–º ‚Äî —Ä–æ–¥–∏—Ç–µ–ª—è –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    if key in seen and _is_parent(c):
                        continue
                    if key not in seen:
                        citations.append({
                            "book": c.get("book", "Unknown"),
                            "section": c.get("section", "Unknown"),
                            "page": c.get("page", 0),
                            "relevance": c.get("score", 0),
                            "is_parent": _is_parent(c),   # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ ‚Äî —É–¥–æ–±–Ω–æ –¥–ª—è UI –±–µ–π–¥–∂–∞ ¬´—Å–µ–∫—Ü–∏—è¬ª
                        })
                        seen.add(key)
                    if len(citations) >= MAX_CITATIONS:
                        break
                
                result = {
                    "answer": final_answer,
                    "citations": citations,
                    "question_type": q_type,
                    "chunks_found": len(chunks),
                    "processing_time": (datetime.now() - start_time).total_seconds(),
                    "model": MODEL,
                    "context_size": context_len,
                    "grounding_score": grounding_score,
                }
                if okpd_hint:
                    result["meta"] = {"okpd_hint": okpd_hint}
                return result
            
        except requests.exceptions.ConnectionError:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama")
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
            return ask_simple(question, chunks=chunks)
            
        except requests.exceptions.Timeout:
            logger.error("Timeout –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama")
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
            return ask_simple(question, chunks=chunks)
            
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            return {
                "answer": f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}",
                "citations": [],
                "error": True,
                "processing_time": (datetime.now() - start_time).total_seconds()
            }
        
        # --- –£–°–ü–ï–®–ù–´–ô –ü–£–¢–¨ (–±–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏–π): —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é, –≥–æ—Ç–æ–≤–∏–º —Ü–∏—Ç–∞—Ç—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç ---
        with self._lock:
            self.conversation_history.extend([
                {"role": "user", "content": question},
                {"role": "assistant", "content": final_answer}
            ])
            if len(self.conversation_history) > 10:
                self.conversation_history = self.conversation_history[-10:]

        citations = []
        seen = set()
        def _is_parent(c: Dict) -> bool:
            return (c.get('is_parent') is True) or ((c.get('meta') or {}).get('is_parent') is True)
        pool = sorted(chunks[:MAX_CITATIONS * 2], key=lambda x: _is_parent(x))
        for c in pool:
            key = (c.get("book", ""), c.get("section", ""), c.get("page", 0))
            if key in seen and _is_parent(c):
                continue
            if key not in seen:
                citations.append({
                    "book": c.get("book", "Unknown"),
                    "section": c.get("section", "Unknown"),
                    "page": c.get("page", 0),
                    "relevance": c.get("score", 0),
                    "is_parent": _is_parent(c),
                })
                seen.add(key)
            if len(citations) >= MAX_CITATIONS:
                break

        result = {
            "answer": final_answer,
            "citations": citations,
            "question_type": q_type,
            "chunks_found": len(chunks),
            "processing_time": (datetime.now() - start_time).total_seconds(),
            "model": MODEL,
            "context_size": len(context),
            "grounding_score": grounding_score,
        }
        if okpd_hint:
            result["meta"] = {"okpd_hint": okpd_hint}
        return result
    
    def clear_history(self):
        """–û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        with self._lock:
            self.conversation_history = []
            self.context_cache = {}

# ---------------------- Smart-Retry helpers ----------------------
_DASHES = "\u2010\u2011\u2012\u2013\u2014\u2212"
_MULT = "√ó"

def normalize_for_retrieval(q: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞: —Ç–∏—Ä–µ, —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ —Ä–∞–∑–º–µ—Ä–æ–≤, –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –∫–µ–π—Å."""
    if not q:
        return ""
    s = q
    # —É–Ω–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Ç–∏—Ä–µ
    s = re.sub(f"[{_DASHES}]", "-", s)
    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ ¬´x/—Ö/√ó/*¬ª –º–µ–∂–¥—É —á–∏—Å–ª–∞–º–∏
    s = re.sub(r'(?<=\d)\s*[x—Ö'+_MULT+r'*]\s*(?=\d)', 'x', s, flags=re.I)
    # —É–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def build_multiquery_expansions(q_norm: str) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ (–ì–û–°–¢, —Ä–∞–∑–º–µ—Ä—ã, –º–∞—Ç–µ—Ä–∏–∞–ª—ã, —Å–∏–Ω–æ–Ω–∏–º—ã —Ñ–æ—Ä–º)."""
    out = [q_norm]
    base = q_norm
    # 1) —Å–ª–µ–ø–ª–µ–Ω–Ω—ã–µ/–¥–µ—Ñ–∏—Å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã (–ü–û–°–ö 50-18 ‚Üí –ü–û–°–ö50-18 / –ü–û–°–ö-50-18)
    base = base.replace("–Å","E").replace("—ë","e")
    out.append(re.sub(r'\s*-\s*', '-', base))
    out.append(re.sub(r'(?<=\b[–ê-–ØA-Z]{2,})\s+(?=\d)', '', base))
    # 2) –≤–∞—Ä–∏–∞–Ω—Ç—ã –ª–∞—Ç/–∫–∏—Ä –¥–ª—è –º–∞—Ä–æ–∫ —Å—Ç–∞–ª–µ–π (12–•18–ù10–¢ ‚Üî 12X18H10T)
    xlat = (base
            .replace("–•","X").replace("—Ö","x")
            .replace("–ù","H").replace("–Ω","h")
            .replace("–°","C").replace("—Å","c"))
    if xlat != base: out.append(xlat)
    # 3) —Å–∏–Ω–æ–Ω–∏–º—ã —Ñ–æ—Ä–º –ø—Ä–æ–∫–∞—Ç–∞
    synonyms = [
        ("—à—Ç—Ä–∏–ø—Å", "–ª–µ–Ω—Ç–∞"), ("–ª–µ–Ω—Ç–∞", "—à—Ç—Ä–∏–ø—Å"),
        ("–ª–∏—Å—Ç", "–ø–ª–∏—Ç–∞"), ("–ø–ª–∏—Ç–∞", "–ª–∏—Å—Ç"),
        ("–ø—Ä—É—Ç–æ–∫", "–∫—Ä—É–≥"), ("–∫—Ä—É–≥", "–ø—Ä—É—Ç–æ–∫"),
        ("–ø—Ä–æ–≤–æ–ª–æ–∫–∞", "–∫–∞—Ç–∞–Ω–∫–∞"), ("–∫–∞—Ç–∞–Ω–∫–∞", "–ø—Ä–æ–≤–æ–ª–æ–∫–∞"),
    ]
    for a,b in synonyms:
        if re.search(rf'\b{a}\b', base, re.I):
            out.append(re.sub(rf'\b{a}\b', b, base, flags=re.I))
    # 4) –ì–û–°–¢-—è–∫–æ—Ä—è (–µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –Ω–æ–º–µ—Ä ‚Äî —Å–æ–∑–¥–∞—ë–º ¬´–ì–û–°–¢ NNNN¬ª –∏ ¬´GOST NNNN¬ª)
    for m in re.findall(r'\b(–ì–û–°–¢|GOST)?\s*([0-9]{3,6})(?:-\d{2,4})?\b', base, flags=re.I):
        num = m[1]
        out += [f"–ì–û–°–¢ {num}", f"GOST {num}"]
    # 5) –∂—ë—Å—Ç–∫–∞—è —Å–∫–ª–µ–π–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ (0.8 x 90 ‚Üí 0.8x90)
    out.append(re.sub(r'\s*x\s*', 'x', base, flags=re.I))
    # de-dup
    seen, uniq = set(), []
    for q in out:
        qn = q.strip()
        if qn and qn.lower() not in seen:
            seen.add(qn.lower())
            uniq.append(qn)
    return uniq[:12]

def retrieve_and_rerank(searcher, queries: List[str], *, topk_per_query: int = 5) -> List[Dict]:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫: –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ ‚Üí –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ ‚Üí –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∫–∞."""
    if not queries:
        return []
    bucket: Dict[str, Dict] = {}
    for q in queries:
        try:
            res = searcher.search(q, top_k=topk_per_query, context_window=1) or []
        except Exception:
            res = []
        for c in res:
            # –∫–ª—é—á: –∏—Å—Ç–æ—á–Ω–∏–∫+—Å—Ç—Ä–∞–Ω–∏—Ü–∞+—Ö—ç—à —Ç–µ–∫—Å—Ç–∞
            raw = (c.get('book',''), c.get('section',''), c.get('page',0), c.get('id',None))
            key = "|".join(map(str, raw)) or hashlib.md5((c.get('text','') or '').encode('utf-8','ignore')).hexdigest()
            prev = bucket.get(key)
            if not prev or float(c.get('score',0)) > float(prev.get('score',0)):
                bucket[key] = c
    merged = list(bucket.values())
    merged.sort(key=lambda x: float(x.get('score',0)), reverse=True)
    return merged

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–∏–Ω–≥–ª—Ç–æ–Ω—ã ---
_searcher_instance: Optional['AdvancedHybridSearch'] = None
_chats: Dict[str, EnhancedRAGChat] = {}
_chats_last_used: Dict[str, float] = {}
_CHATS_LOCK = threading.RLock()
_CHATS_MAX = int(os.getenv("RAG_MAX_CONV", "200"))        # –ª–∏–º–∏—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π
_CHATS_TTL_SEC = int(os.getenv("RAG_CONV_TTL_SEC", "3600"))  # –∞–≤—Ç–æ-–æ—á–∏—Å—Ç–∫–∞ –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö (—Å–µ–∫)

def _get_searcher_singleton() -> 'AdvancedHybridSearch':
    global _searcher_instance
    if _searcher_instance is None:
        from search_windows import get_searcher
        _searcher_instance = get_searcher()
    return _searcher_instance

def _prune_chats(now: Optional[float] = None) -> None:
    """–û–ø–ø–æ—Ä—Ç—É–Ω–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —á–∏—Å—Ç–∫–∞: –ø–æ TTL –∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É."""
    if now is None:
        now = time.time()
    to_drop = []
    # TTL
    for cid, ts in list(_chats_last_used.items()):
        if now - ts > _CHATS_TTL_SEC:
            to_drop.append(cid)
    # –ø–æ —Ä–∞–∑–º–µ—Ä—É
    if len(_chats) - len(to_drop) > _CHATS_MAX:
        # –≤—ã–±—Ä–æ—Å–∏–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ —Å–Ω–∞—á–∞–ª–∞
        survivors = {cid: _chats_last_used[cid] for cid in _chats if cid not in to_drop}
        overflow = len(_chats) - _CHATS_MAX - len(to_drop)
        if overflow > 0:
            for cid, _ in sorted(survivors.items(), key=lambda kv: kv[1])[:overflow]:
                to_drop.append(cid)
    # –ø—Ä–∏–º–µ–Ω—è–µ–º
    for cid in to_drop:
        _chats.pop(cid, None)
        _chats_last_used.pop(cid, None)

def _get_chat(conversation_id: Optional[str]) -> EnhancedRAGChat:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∞—Ç –¥–ª—è —Å–µ—Å—Å–∏–∏. –ï—Å–ª–∏ conversation_id –Ω–µ –∑–∞–¥–∞–Ω ‚Äî –æ–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–π —á–∞—Ç –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è."""
    searcher = _get_searcher_singleton()
    if conversation_id is None or str(conversation_id).strip() == "":
        return EnhancedRAGChat(searcher)  # ephemeral: –Ω–µ –∫—ç—à–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –º–µ–∂–¥—É –≤—ã–∑–æ–≤–∞–º–∏

    cid = str(conversation_id)[:128]
    now = time.time()
    with _CHATS_LOCK:
        chat = _chats.get(cid)
        if chat is None:
            chat = EnhancedRAGChat(searcher)
            _chats[cid] = chat
        _chats_last_used[cid] = now
        _prune_chats(now)
        return chat

def ask(
    question: str,
    top_k: int = 10,
    chunks: Optional[List[Dict]] = None,
    *,
    conversation_id: Optional[str] = None,
    qa_id: Optional[str] = None
) -> Dict:
    """–û—Å–Ω–æ–≤–Ω–æ–π –≤—Ö–æ–¥: —á–∞—Ç –ø—Ä–∏–≤—è–∑—ã–≤–∞–µ—Ç—Å—è –∫ conversation_id. –ë–µ–∑ id ‚Äî –æ–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–π —á–∞—Ç."""
    chat = _get_chat(conversation_id)
    effective_top_k = top_k if top_k is not None else MAX_FRAGMENTS
    return chat.generate_answer(
        question,
        effective_top_k,
        chunks=chunks,
        qa_id=qa_id,
        conversation_id=conversation_id,
    )

def clear_conversation(conversation_id: str) -> bool:
    """–°—Ç–µ—Ä–µ—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å–µ—Å—Å–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)."""
    with _CHATS_LOCK:
        conversation_id = str(conversation_id)[:128]
        chat = _chats.get(conversation_id)
        if chat:
            chat.clear_history()
            _chats_last_used[conversation_id] = time.time()
            return True
        return False

def drop_conversation(conversation_id: str) -> bool:
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é —É–¥–∞–ª–∏—Ç—å —Å–µ—Å—Å–∏—é –∏–∑ –∫—ç—à–∞."""
    conversation_id = str(conversation_id)[:128]
    with _CHATS_LOCK:
        existed = _chats.pop(conversation_id, None) is not None
        _chats_last_used.pop(conversation_id, None)
        return existed

def ask_simple(question: str, top_k: int = 5, chunks: Optional[List[Dict]] = None, qa_id: Optional[str] = None, conversation_id: Optional[str] = None) -> Dict:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ LLM - –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"""
    log = CtxLog(logger, {"qa_id": qa_id, "cid": conversation_id})
    log.debug(f"ask_simple start: top_k={top_k}, chunks_in={len(chunks) if chunks else 0}")

    # –ï—Å–ª–∏ chunks –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã, –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∏—Ö
    if chunks is None:
        from search_windows import search
        chunks = search(question, top=top_k)
    log.debug(f"ask_simple chunks={len(chunks) if chunks else 0}")
    
    if not chunks:
        return {
            "answer": "–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.",
            "citations": [],
            "grounding_score": 0.0
        }
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    answer = "## –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:\n\n"
    
    for i, chunk in enumerate(chunks[:top_k], 1):
        answer += f"### {i}. {chunk.get('book', 'Unknown')} - {chunk.get('section', 'Unknown')}\n"
        answer += f"*–°—Ç—Ä–∞–Ω–∏—Ü–∞ {chunk.get('page', 0)} | –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {chunk.get('score', 0):.2%}*\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–≤—å—é —Ç–µ–∫—Å—Ç–∞
        text = chunk.get('text', '')
        text_preview = text[:500]
        if len(text) > 500:
            text_preview += "..."
        
        answer += f"{text_preview}\n\n"
        answer += "---\n\n"
    
    citations = [
        {
            "book": c.get("book", "Unknown"),
            "section": c.get("section", "Unknown"), 
            "page": c.get("page", 0),
            "relevance": c.get("score", 0)
        }
        for c in chunks[:top_k]
    ]
    
    return {
        "answer": answer,
        "citations": citations,
        "chunks_found": len(chunks),
        "grounding_score": 1.0  # –ü—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –≤—Å–µ–≥–¥–∞ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
    }

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç')
    parser.add_argument('question', nargs='*', help='–í–∞—à –≤–æ–ø—Ä–æ—Å')
    parser.add_argument('--simple', action='store_true', help='–ü—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º –±–µ–∑ LLM')
    parser.add_argument('--top-k', type=int, default=MAX_FRAGMENTS, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤')
    parser.add_argument('--no-cot', action='store_true', help='–û—Ç–∫–ª—é—á–∏—Ç—å Chain of Thought')
    parser.add_argument('--clear-history', action='store_true', help='–û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é')
    
    args = parser.parse_args()
    
    # –î–ª—è CLI –≤–µ—Ä—Å–∏–∏ —Å–æ–∑–¥–∞–µ–º chat —Å searcher
    from search_windows import AdvancedHybridSearch, get_searcher
    searcher = get_searcher()
    chat = EnhancedRAGChat(searcher)
    
    if args.clear_history:
        chat.clear_history()
        print("–ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –æ—á–∏—â–µ–Ω–∞.\n")
    
    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    if not args.question:
        print("=" * 80)
        print("–ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú RAG CHAT")
        print("=" * 80)
        print("–í–≤–µ–¥–∏—Ç–µ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞, 'clear' –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏\n")
        
        while True:
            try:
                q = input("\nüìù –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
                
                if q.lower() == 'exit':
                    break
                elif q.lower() == 'clear':
                    chat.clear_history()
                    print("‚úì –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞")
                    continue
                elif not q:
                    continue
                
                print("\n‚è≥ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞...\n")
                
                if args.simple:
                    result = ask_simple(q, top_k=args.top_k)
                else:
                    result = chat.generate_answer(
                        q, 
                        top_k=args.top_k,
                        use_cot=not args.no_cot
                    )
                
                # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                print("=" * 80)
                print("üí° –û–¢–í–ï–¢:")
                print("=" * 80)
                print(result["answer"])
                
                if result.get("citations"):
                    print("\n" + "=" * 80)
                    print("üìö –ò–°–¢–û–ß–ù–ò–ö–ò:")
                    print("=" * 80)
                    for i, cit in enumerate(result["citations"], 1):
                        relevance = cit.get('relevance', 0)
                        print(f"{i}. {cit['book']} - {cit['section']} (—Å—Ç—Ä. {cit['page']}) [{relevance:.1%}]")
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                if 'processing_time' in result:
                    print(f"\n‚ö° –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result['processing_time']:.2f} —Å–µ–∫")
                if 'grounding_score' in result:
                    print(f"‚úÖ –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏: {result['grounding_score']:.1%}")
                if 'question_type' in result:
                    print(f"üè∑Ô∏è –¢–∏–ø –≤–æ–ø—Ä–æ—Å–∞: {result['question_type']}")
                if 'chunks_found' in result:
                    print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {result['chunks_found']}")
                
                # OKPD hint –µ—Å–ª–∏ –µ—Å—Ç—å
                if result.get("meta", {}).get("okpd_hint"):
                    hint = result["meta"]["okpd_hint"]
                    print(f"üîç OKPD –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: {hint.get('code')} ({hint.get('conf', 0):.0%})")
                    
            except KeyboardInterrupt:
                print("\n\n–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
                break
            except Exception as e:
                print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
                continue
    
    else:
        # –û–¥–∏–Ω–æ—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        q = " ".join(args.question)
        
        print(f"\nüìù –í–æ–ø—Ä–æ—Å: {q}\n")
        print("‚è≥ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...\n")
        
        if args.simple:
            result = ask_simple(q, top_k=args.top_k)
        else:
            result = chat.generate_answer(
                q,
                top_k=args.top_k,
                use_cot=not args.no_cot
            )
        
        print("=" * 80)
        print("üí° –û–¢–í–ï–¢:")
        print("=" * 80)
        print(result["answer"])
        
        if result.get("citations"):
            print("\n" + "=" * 80)
            print("üìö –ò–°–¢–û–ß–ù–ò–ö–ò:")
            print("=" * 80)
            for i, cit in enumerate(result["citations"], 1):
                print(f"{i}. {cit['book']} - {cit['section']} (—Å—Ç—Ä. {cit['page']})")
        
        # OKPD hint –µ—Å–ª–∏ –µ—Å—Ç—å
        if result.get("meta", {}).get("okpd_hint"):
            hint = result["meta"]["okpd_hint"]
            print(f"\nüîç OKPD –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: {hint.get('code')} ({hint.get('conf', 0):.0%})")